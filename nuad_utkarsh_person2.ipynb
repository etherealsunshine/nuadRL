{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfSf4XfxYnb5",
        "outputId": "70343ce9-4420-4ea6-ece3-ecd7c24b2cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBIN4Ornlppt",
        "outputId": "f744a6d1-3c90-4f56-ad76-a0c2cae1198a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['Screenshot 2024-07-18 at 5.25.47\\u202fPM.png', 'Senior Year Transcript.pdf', 'His 072B', 'Paper 3-2.pdf', 'Paper 3-2.gdoc', 'SAS 002 Essay.gdoc', 'Successful Scheduling (2).png', 'Study Guide Wage Labor.gdoc', 'Colab Notebooks', 'Kidney_marker_gene.xlsx', 'History Paper 2.gdoc', 'History Paper 2 .docx', 'History Paper 2 with comments.docx', 'output_file.h5ad', 'SAS Essay 2.gdoc', 'bioinformaticscopilot_CosMx_v11.ipynb', 'HIS 072 .gdoc', 'ECS 17.gdoc', 'resume_v1.pdf', 'SAS 005 (1).gdoc', 'Untitled document (1).gdoc', 'ECS 17 Notes.gdoc', 'SAS 005_2.gdoc', 'SAS 005.gdoc', 'Copy of SAS 005.gdoc', 'DDSC Project proposal.gdoc', 'Untitled document.gdoc', '2023-08-21_Preprocess_Practice.ipynb', 'resume (2).pdf', 'resume (1).pdf', 'COM 4 Art Presentation.gdoc', 'COM 4.gdoc', 'SAS 5 essay 2.gdoc', 'resume.pdf', 'essay 2.gdoc', 'LOR.gdoc', 'Gantt chart.gsheet', 'Project timeline.gsheet', 'Life Tracker.gsheet', 'Resume.pdf', 'AME 1E.gdoc', 'OfferLetter_Utkarsh Singh (1).pdf', 'OfferLetter_Utkarsh Singh.pdf', 'SAS 5.gdoc', 'AMS Annotated BiblioGraphy.gdoc', 'ECS 32A.gdoc', 'package']\n",
            "CSV files in the directory:\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Path to your directory in Google Drive\n",
        "data_path = '/content/drive/My Drive/'\n",
        "\n",
        "# List the contents of the directory\n",
        "print(os.listdir(data_path))\n",
        "\n",
        "print(\"CSV files in the directory:\")\n",
        "for filename in os.listdir(data_path):\n",
        "    if filename.endswith('.csv'):\n",
        "        print(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpPYkU9roDCa",
        "outputId": "414b78ea-c642-4ef2-862f-39bc75d04d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: /content/drive/MyDrive/package\n",
            "Requirement already satisfied: nupack in /usr/local/lib/python3.11/dist-packages (4.0.1.12)\n",
            "Requirement already satisfied: pyyaml>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nupack) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from nupack) (1.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from nupack) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from nupack) (2.2.2)\n",
            "Requirement already satisfied: jinja2>=2.0 in /usr/local/lib/python3.11/dist-packages (from nupack) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.0->nupack) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->nupack) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->nupack) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.0->nupack) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.0->nupack) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install -U nupack --no-index -f \"/content/drive/MyDrive/package\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqsnZdGRn-k1",
        "outputId": "10450d45-c508-494a-b519-c31b125ba0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nuad in /usr/local/lib/python3.11/dist-packages (0.4.6)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nuad) (5.9.5)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.11/dist-packages (from nuad) (4.1.0)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.11/dist-packages (from nuad) (0.3.4)\n",
            "Requirement already satisfied: nupack in /usr/local/lib/python3.11/dist-packages (from nuad) (4.0.1.12)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from nuad) (0.9.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from nuad) (3.1.5)\n",
            "Requirement already satisfied: scadnano in /usr/local/lib/python3.11/dist-packages (from nuad) (0.20.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from nupack->nuad) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from nupack->nuad) (1.15.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->nuad) (2.0.0)\n",
            "Requirement already satisfied: ppft>=1.7.7 in /usr/local/lib/python3.11/dist-packages (from pathos->nuad) (1.7.7)\n",
            "Requirement already satisfied: dill>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pathos->nuad) (0.4.0)\n",
            "Requirement already satisfied: pox>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from pathos->nuad) (0.3.6)\n",
            "Requirement already satisfied: multiprocess>=0.70.18 in /usr/local/lib/python3.11/dist-packages (from pathos->nuad) (0.70.18)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install nuad gym stable-baselines3 torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inLdovcA0t_v",
        "outputId": "9c3fbcd4-6732-4c1b-ecaa-4ad97eec1ea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,926 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,720 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,211 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,901 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,546 kB]\n",
            "Fetched 20.9 MB in 3s (7,498 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libgsl27 libgslcblas0 libncbi6 ncbi-data readseq\n",
            "Suggested packages:\n",
            "  gsl-ref-psdoc | gsl-doc-pdf | gsl-doc-info | gsl-ref-html\n",
            "The following NEW packages will be installed:\n",
            "  libgsl27 libgslcblas0 libncbi6 ncbi-data readseq vienna-rna\n",
            "0 upgraded, 6 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 11.1 MB of archives.\n",
            "After this operation, 130 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgslcblas0 amd64 2.7.1+dfsg-3 [94.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgsl27 amd64 2.7.1+dfsg-3 [1,000 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ncbi-data all 6.1.20170106+dfsg1-9 [3,519 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libncbi6 amd64 6.1.20170106+dfsg1-9 [3,992 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 readseq amd64 1-14 [55.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 vienna-rna amd64 2.4.17+dfsg-2build2 [2,469 kB]\n",
            "Fetched 11.1 MB in 1s (8,647 kB/s)\n",
            "Selecting previously unselected package libgslcblas0:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgslcblas0_2.7.1+dfsg-3_amd64.deb ...\n",
            "Unpacking libgslcblas0:amd64 (2.7.1+dfsg-3) ...\n",
            "Selecting previously unselected package libgsl27:amd64.\n",
            "Preparing to unpack .../1-libgsl27_2.7.1+dfsg-3_amd64.deb ...\n",
            "Unpacking libgsl27:amd64 (2.7.1+dfsg-3) ...\n",
            "Selecting previously unselected package ncbi-data.\n",
            "Preparing to unpack .../2-ncbi-data_6.1.20170106+dfsg1-9_all.deb ...\n",
            "Unpacking ncbi-data (6.1.20170106+dfsg1-9) ...\n",
            "Selecting previously unselected package libncbi6:amd64.\n",
            "Preparing to unpack .../3-libncbi6_6.1.20170106+dfsg1-9_amd64.deb ...\n",
            "Unpacking libncbi6:amd64 (6.1.20170106+dfsg1-9) ...\n",
            "Selecting previously unselected package readseq.\n",
            "Preparing to unpack .../4-readseq_1-14_amd64.deb ...\n",
            "Unpacking readseq (1-14) ...\n",
            "Selecting previously unselected package vienna-rna.\n",
            "Preparing to unpack .../5-vienna-rna_2.4.17+dfsg-2build2_amd64.deb ...\n",
            "Unpacking vienna-rna (2.4.17+dfsg-2build2) ...\n",
            "Setting up ncbi-data (6.1.20170106+dfsg1-9) ...\n",
            "Setting up libgslcblas0:amd64 (2.7.1+dfsg-3) ...\n",
            "Setting up libgsl27:amd64 (2.7.1+dfsg-3) ...\n",
            "Setting up libncbi6:amd64 (6.1.20170106+dfsg1-9) ...\n",
            "Setting up vienna-rna (2.4.17+dfsg-2build2) ...\n",
            "Setting up readseq (1-14) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install vienna-rna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cjeZawUhoeCq"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import argparse\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "import nuad.constraints as nc\n",
        "import nuad.search as ns\n",
        "\n",
        "\n",
        "# DNA sequence designer for a 2D canvas of single-stranded tiles (SSTs).\n",
        "# See docstring for create_design for a detailed description of the design.\n",
        "\n",
        "def main() -> None:\n",
        "    args: CLArgs = parse_command_line_arguments()\n",
        "\n",
        "    design = create_design(width=args.width, height=args.height)\n",
        "\n",
        "    constraints = create_constraints(design)\n",
        "\n",
        "    params = ns.SearchParameters(\n",
        "        constraints=constraints,\n",
        "        out_directory=args.directory,\n",
        "        restart=args.restart,\n",
        "        random_seed=args.seed,\n",
        "        scrolling_output=False,\n",
        "        save_report_for_all_updates=True,\n",
        "        force_overwrite=args.force_overwrite,\n",
        "        # log_time=True,\n",
        "    )\n",
        "    ns.search_for_sequences(design, params)\n",
        "\n",
        "\n",
        "# command-line arguments\n",
        "@dataclass\n",
        "class CLArgs:\n",
        "    directory: str\n",
        "    \"\"\"output directory for search\"\"\"\n",
        "\n",
        "    restart: bool\n",
        "    \"\"\"whether to restart a stopped search\"\"\"\n",
        "\n",
        "    width: int\n",
        "    \"\"\"width of SST canvas\"\"\"\n",
        "\n",
        "    height: int\n",
        "    \"\"\"height of SST canvas\"\"\"\n",
        "\n",
        "    seed: Optional[int] = None\n",
        "    \"\"\"seed for random number generator; set to fixed integer for reproducibility\"\"\"\n",
        "\n",
        "    force_overwrite: bool = False\n",
        "    \"\"\"whether to overwrite output files without prompting the user\"\"\"\n",
        "\n",
        "\n",
        "def parse_command_line_arguments() -> CLArgs:\n",
        "    default_directory = os.path.join('output', ns.script_name_no_ext())\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Designs DNA sequences for a canvas of single-stranded tiles (SSTs).',\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "\n",
        "    parser.add_argument('-o', '--output-dir', type=str, default=default_directory,\n",
        "                        help='directory in which to place output files')\n",
        "\n",
        "    parser.add_argument('-s', '--seed', type=int,\n",
        "                        help='seed for random number generator')\n",
        "\n",
        "    parser.add_argument('-w', '--width', type=int, required=True,\n",
        "                        help='width of canvas (number of tiles)')\n",
        "\n",
        "    parser.add_argument('-ht', '--height', type=int, required=True,\n",
        "                        help='height of canvas (number of tiles)')\n",
        "\n",
        "    parser.add_argument('-r', '--restart', action='store_true',\n",
        "                        help='If true, then assumes output directory contains output of search that was '\n",
        "                             'cancelled, to restart from. Will automatically find the most recent design '\n",
        "                             '(assuming they are indexed with a number such as 84), and will start the '\n",
        "                             'numbering from there (i.e., the next files to be written upon improving the '\n",
        "                             'design will have index 85).')\n",
        "\n",
        "    parser.add_argument('-f', '--force', action='store_true',\n",
        "                        help='If true, then overwrites the output files without prompting the user.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return CLArgs(\n",
        "        directory=args.output_dir,\n",
        "        width=args.width,\n",
        "        height=args.height,\n",
        "        seed=args.seed,\n",
        "        restart=args.restart,\n",
        "        force_overwrite=args.force,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_design(width: int, height: int) -> nc.Design:\n",
        "    \"\"\"\n",
        "    Creates an SST canvas `width` tiles width and `height` tiles high.\n",
        "\n",
        "    For instance a width=4 x height=3 canvas looks like below, with each tile named t_x_y.\n",
        "    x goes from 0 up to `width`-1.\n",
        "    y goes from 0 up to `height`-1.\n",
        "\n",
        "    Domain lengths are listed at the top in the next figure.\n",
        "\n",
        "    .. code-block:: none\n",
        "\n",
        "        |     10    |     11      |     10     |     11      |     10     |     11      |     10     |\n",
        "\n",
        "                                   +==========---===========>\n",
        "                                   |         t_0_2\n",
        "                                   +==========---===========]\n",
        "                     +===========---==========> +===========---==========>\n",
        "                     |          t_0_1           |          t_1_2\n",
        "                     +===========---==========] +===========---==========]\n",
        "        +==========---===========> +==========---===========> +==========---===========>\n",
        "        |         t_0_0            |         t_1_1            |         t_2_2\n",
        "        +==========---===========] +==========---===========] +==========---===========]\n",
        "                     +===========---==========> +===========---==========> +===========---==========>\n",
        "                     |          t_1_0           |          t_2_1           |          t_3_2\n",
        "                     +===========---==========] +===========---==========] +===========---==========]\n",
        "                                   +==========---===========> +==========---===========>\n",
        "                                   |         t_2_0            |         t_3_1\n",
        "                                   +==========---===========] +==========---===========]\n",
        "                                                +===========---==========>\n",
        "                                                |          t_3_0\n",
        "                                                +===========---==========]\n",
        "\n",
        "    :param width:\n",
        "        number of tiles wide to make canvas\n",
        "    :param height:\n",
        "        number of tiles high to make canvas\n",
        "    :return:\n",
        "        design with `width` x `height` canvas of SSTs\n",
        "    \"\"\"\n",
        "    numpy_filters = [\n",
        "        nc.NearestNeighborEnergyFilter(-9.3, -9.0, 52.0),  # energies should all be \"close\"\n",
        "        nc.RunsOfBasesFilter(['C', 'G'], 4),  # forbid substrings of form {C,G}^4\n",
        "        nc.ForbiddenSubstringFilter(['AAAAA', 'TTTTT']),  # forbid 5 A's in a row or 5 T's in a row\n",
        "    ]\n",
        "\n",
        "    domain_pool_10 = nc.DomainPool(f'length-10_domains', 10, numpy_filters=numpy_filters)\n",
        "    domain_pool_11 = nc.DomainPool(f'length-11_domains', 11, numpy_filters=numpy_filters)\n",
        "\n",
        "    design = nc.Design()\n",
        "\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            # domains are named after the strand for which they are on the bottom,\n",
        "            # so the two domains on top are starred and named after the tiles to which they bind\n",
        "            # If you tilt your head 45 degrees left, then glues are\n",
        "            # \"north\" (n), \"south\" (s), \"west\" (w), \"east\" (e),\n",
        "            # so start with either ns_ (\"north-south\") or we_ (\"west-east\")\n",
        "            # From 5' ] to 3' >, they are in the order s, w, n, e.\n",
        "            #\n",
        "            # Parity of x+y determines whether first and last domain (s and e) are length 11 or 10.\n",
        "            #\n",
        "            # e.g. this is tile t_3_5:\n",
        "            #\n",
        "            #          10           11\n",
        "            #     +==========--===========>\n",
        "            #     |  ns_2_5*     we_3_6*\n",
        "            #     |\n",
        "            #     |  we_3_5      ns_3_5\n",
        "            #     +==========--===========]\n",
        "            #\n",
        "            # and this is tile t_6_1:\n",
        "            #\n",
        "            #          11           10\n",
        "            #     +===========--==========>\n",
        "            #     |  ns_5_1*      we_6_2*\n",
        "            #     |\n",
        "            #     |  we_6_1       ns_6_1\n",
        "            #     +===========--==========]\n",
        "\n",
        "            s_domain_name = f'ns_{x}_{y}'\n",
        "            w_domain_name = f'we_{x}_{y}'\n",
        "            n_domain_name = f'ns_{x - 1}_{y}*'\n",
        "            e_domain_name = f'we_{x}_{y + 1}*'\n",
        "            tile = design.add_strand(\n",
        "                domain_names=[s_domain_name, w_domain_name, n_domain_name, e_domain_name], name=f't_{x}_{y}')\n",
        "\n",
        "            if (x + y) % 2 == 0:\n",
        "                outer_pool = domain_pool_11\n",
        "                inner_pool = domain_pool_10\n",
        "            else:\n",
        "                outer_pool = domain_pool_10\n",
        "                inner_pool = domain_pool_11\n",
        "\n",
        "            s_domain, w_domain, n_domain, e_domain = tile.domains\n",
        "            if not s_domain.has_pool():\n",
        "                s_domain.pool = outer_pool\n",
        "            if not e_domain.has_pool():\n",
        "                e_domain.pool = outer_pool\n",
        "            if not n_domain.has_pool():\n",
        "                n_domain.pool = inner_pool\n",
        "            if not w_domain.has_pool():\n",
        "                w_domain.pool = inner_pool\n",
        "\n",
        "    return design\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Thresholds:\n",
        "    temperature: float = 52.0\n",
        "    \"\"\"Temperature in Celsius\"\"\"\n",
        "\n",
        "    tile_ss: float = -1.5\n",
        "    \"\"\"NUPACK complex free energy threshold for individual tiles.\"\"\"\n",
        "\n",
        "    tile_pair_0comp: float = -2.5\n",
        "    \"\"\"RNAduplex complex free energy threshold for pairs tiles with no complementary domains.\"\"\"\n",
        "\n",
        "    tile_pair_1comp: float = -6.5\n",
        "    \"\"\"RNAduplex complex free energy threshold for pairs tiles with 1 complementary domain.\"\"\"\n",
        "\n",
        "\n",
        "def create_constraints(design: nc.Design) -> List[nc.Constraint]:\n",
        "    thresholds = Thresholds()\n",
        "\n",
        "    strand_individual_ss_constraint = nc.nupack_strand_free_energy_constraint(\n",
        "    threshold=thresholds.tile_ss, temperature=thresholds.temperature, short_description='StrandSS')\n",
        "\n",
        "    strand_pairs_rna_duplex_constraint_0comp, strand_pairs_rna_duplex_constraint_1comp = \\\n",
        "    nc.rna_duplex_strand_pairs_constraints_by_number_matching_domains(\n",
        "        thresholds={0: thresholds.tile_pair_0comp,\n",
        "                    1: thresholds.tile_pair_1comp},\n",
        "        temperature=thresholds.temperature,\n",
        "        short_descriptions={0: 'StrandPairRNA0Comp',\n",
        "                            1: 'StrandPairRNA1Comp'},\n",
        "        strands=design.strands,\n",
        "        ignore_missing_thresholds=True,   # ← allow missing k’s\n",
        "    )\n",
        "\n",
        "    no_gggg_constraint = create_tile_no_gggg_constraint(weight=100)\n",
        "\n",
        "    return [\n",
        "        strand_individual_ss_constraint,\n",
        "        strand_pairs_rna_duplex_constraint_0comp,\n",
        "        strand_pairs_rna_duplex_constraint_1comp,\n",
        "        no_gggg_constraint,\n",
        "    ]\n",
        "\n",
        "\n",
        "def create_tile_no_gggg_constraint(weight: float) -> nc.StrandConstraint:\n",
        "    # This shows how one might make a custom constraint, in case those in dsd.constraints are not\n",
        "    # sufficient. See also source code of provided constraints in dsd/constraints.py for more examples,\n",
        "    # particularly for examples that call NUPACK or ViennaRNA.\n",
        "\n",
        "    # We already forbid GGGG in any domain, but let's also ensure we don't get GGGG in any strand\n",
        "    # i.e., forbid GGGG that comes from concatenating domains, e.g.,\n",
        "    #\n",
        "    #               *  ***\n",
        "    #      ACGATCGATG  GGGATGCATGA\n",
        "    #     +==========--===========>\n",
        "    #     |\n",
        "    #     +==========--===========]\n",
        "\n",
        "    def evaluate(seqs: Tuple[str, ...], strand: Optional[nc.Strand]) -> nc.Result:  # noqa\n",
        "        sequence = seqs[0]\n",
        "        if 'GGGG' in sequence:\n",
        "            result = nc.Result(excess=1.0, summary=f'GGGG found in {sequence}', value=sequence.count('GGGG'))\n",
        "        else:\n",
        "            result = nc.Result(excess=0.0, summary=f'', value=0)\n",
        "        return result\n",
        "\n",
        "    description = \"No GGGG allowed in strand's sequence\"\n",
        "\n",
        "    return nc.StrandConstraint(description=description, short_description='NoGGGG',\n",
        "                               weight=weight, evaluate=evaluate)\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e74yHDRR5gMk",
        "outputId": "1a451356-6a65-4f9a-c1f1-7bdc227b7d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-09 22:46:55--  https://raw.githubusercontent.com/UC-Davis-molecular-computing/nuad/main/examples/sst_canvas.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10674 (10K) [text/plain]\n",
            "Saving to: ‘sst_canvas.py’\n",
            "\n",
            "sst_canvas.py       100%[===================>]  10.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-09 22:46:55 (80.2 MB/s) - ‘sst_canvas.py’ saved [10674/10674]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/UC-Davis-molecular-computing/nuad/main/examples/sst_canvas.py -O sst_canvas.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL1xmH2U5gkx",
        "outputId": "d706e930-4f8b-4707-f787-17c4aa0644c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using random seed of 42; use this same seed to reproduce this run\n",
            "number of processes in system: 1\n",
            "|-----------|--------|-----------|-----------|----------|--------------------|--------------------|--------|\n",
            "| iteration | update | opt score | new score | StrandSS | StrandPairRNA0Comp | StrandPairRNA1Comp | NoGGGG |\n",
            "|      2793 |    133 |     0.212 |     0.786 |    0.209 |            0.00206 |           0.001000 |    0.0 |\n",
            "We've generated all possible DNA sequences at Hamming distance 1 \n",
            "from the previous sequence TCTCTAGGGA and not found one that passed your \n",
            "NumpyFilters and SequenceFilters. Trying another distance.\n",
            "|      3219 |    137 |     0.203 |     0.203 |    0.200 |            0.00206 |            0.00100 |    0.0 |\n",
            "We've generated all possible DNA sequences at Hamming distance 1 \n",
            "from the previous sequence TTCGTTTTGTA and not found one that passed your \n",
            "NumpyFilters and SequenceFilters. Trying another distance.\n",
            "|      3328 |    137 |     0.203 |     0.204 |    0.200 |            0.00206 |            0.00100 |    0.0 |\n",
            "We've generated all possible DNA sequences at Hamming distance 1 \n",
            "from the previous sequence TTCGTTTTGTA and not found one that passed your \n",
            "NumpyFilters and SequenceFilters. Trying another distance.\n",
            "|      3344 |    137 |     0.203 |     0.311 |    0.200 |            0.00206 |            0.00100 |    0.0 |\n",
            "We've generated all possible DNA sequences at Hamming distance 1 \n",
            "from the previous sequence TTCGTTTTGTA and not found one that passed your \n",
            "NumpyFilters and SequenceFilters. Trying another distance.\n",
            "|      9562 |    193 |   0.00351 |    0.0287 |  0.00144 |           0.000343 |            0.00173 |    0.0 |\n",
            "We've generated all possible DNA sequences at Hamming distance 1 \n",
            "from the previous sequence GGTTGTTGTT and not found one that passed your \n",
            "NumpyFilters and SequenceFilters. Trying another distance.\n",
            "|      9570 |    193 |   0.00351 |    0.0776 |  0.00144 |           0.000343 |            0.00173 |    0.0 |\n",
            "We've generated all possible DNA sequences at Hamming distance 1 \n",
            "from the previous sequence GGTTGTTGTT and not found one that passed your \n",
            "NumpyFilters and SequenceFilters. Trying another distance.\n",
            "|     12811 |    228 |      -0.0 |      -0.0 |      0.0 |                0.0 |                0.0 |    0.0 |\n"
          ]
        }
      ],
      "source": [
        "!python sst_canvas.py \\\n",
        "  --width 4 \\\n",
        "  --height 3 \\\n",
        "  --output-dir ./demo_output \\\n",
        "  --seed 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PfprnOiodK-"
      },
      "outputs": [],
      "source": [
        "#from sst_designer import create_design, create_constraints\n",
        "import nuad.search as ns\n",
        "\n",
        "# 1) build a tiny 1×1 design\n",
        "design = create_design(1, 1)\n",
        "constraints = create_constraints(design)   # no more ValueError!\n",
        "\n",
        "\n",
        "# 2) point search_for_sequences at a temp folder\n",
        "params = ns.SearchParameters(\n",
        "    constraints=constraints,\n",
        "    out_directory='tmp_1x1',\n",
        "    random_seed=42,\n",
        "    scrolling_output=True,         # live stdout of progress\n",
        "    save_report_for_all_updates=True,\n",
        "    force_overwrite=True\n",
        ")\n",
        "ns.search_for_sequences(design, params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ynBQmLpclO9"
      },
      "source": [
        "## Simulate a Mock Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y5AwT38Abhrg"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "\n",
        "# Mock NUAD Environment for development purposes\n",
        "class MockNUADEnv(gym.Env):\n",
        "    def __init__(self, sequence_length=30, num_constraints=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_constraints = num_constraints\n",
        "\n",
        "        # Define observation space: sequence (one-hot encoded) and constraint values\n",
        "        # Nucleotides represented as integers 0-3 (A, C, G, T)\n",
        "        self.observation_space = spaces.Dict({\n",
        "            'sequence': spaces.Box(low=0, high=1, shape=(4, sequence_length), dtype=np.float32),\n",
        "            'constraint_values': spaces.Box(low=-10, high=10, shape=(num_constraints,), dtype=np.float32)\n",
        "        })\n",
        "\n",
        "        # Action space: (position, new_nucleotide)\n",
        "        self.action_space = spaces.MultiDiscrete([sequence_length, 4])\n",
        "\n",
        "        # Internal state\n",
        "        self.current_sequence = None\n",
        "        self.current_constraints = None\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Generate random sequence\n",
        "        self.current_sequence = np.random.randint(0, 4, size=self.sequence_length)\n",
        "\n",
        "        # Generate random constraint values\n",
        "        self.current_constraints = np.random.uniform(-5, 5, size=self.num_constraints)\n",
        "\n",
        "        # Return observation\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # Unpack action\n",
        "        position, nucleotide = action\n",
        "\n",
        "        # Apply action\n",
        "        old_nucleotide = self.current_sequence[position]\n",
        "        self.current_sequence[position] = nucleotide\n",
        "\n",
        "        # Calculate simple reward based on position and nucleotide\n",
        "        # In the real environment, this would come from NUAD's constraint evaluation\n",
        "        reward_change = 0\n",
        "\n",
        "        # Simulate constraints changing based on the nucleotide change\n",
        "        for i in range(self.num_constraints):\n",
        "            # Random but deterministic change to constraints\n",
        "            old_value = self.current_constraints[i]\n",
        "            # More realistic: different constraints affected differently by the change\n",
        "            if i % 4 == nucleotide:\n",
        "                change = -0.5  # Improvement\n",
        "            elif i % 4 == old_nucleotide:\n",
        "                change = 0.3   # Worsening\n",
        "            else:\n",
        "                change = 0.1 * np.sin(position * i)  # Small change\n",
        "\n",
        "            self.current_constraints[i] += change\n",
        "            reward_change -= change  # Negative because lower constraint value is better\n",
        "\n",
        "        # Get new observation\n",
        "        observation = self._get_observation()\n",
        "\n",
        "        # Determine if episode is done (simplified)\n",
        "        done = all(c < -4.0 for c in self.current_constraints)\n",
        "\n",
        "        # Additional info\n",
        "        info = {\n",
        "            'constraint_values': self.current_constraints,\n",
        "            'constraint_improvement': reward_change\n",
        "        }\n",
        "\n",
        "        return observation, reward_change, done, False, info\n",
        "\n",
        "    def _get_observation(self):\n",
        "        # Convert sequence to one-hot encoding\n",
        "        one_hot = np.zeros((4, self.sequence_length), dtype=np.float32)\n",
        "        for i, nucleotide in enumerate(self.current_sequence):\n",
        "            one_hot[nucleotide, i] = 1\n",
        "\n",
        "        return {\n",
        "            'sequence': one_hot,\n",
        "            'constraint_values': self.current_constraints.astype(np.float32)\n",
        "        }\n",
        "\n",
        "    def render(self):\n",
        "        # Simple rendering of the current sequence\n",
        "        nucleotides = ['A', 'C', 'G', 'T']\n",
        "        sequence_str = ''.join(nucleotides[n] for n in self.current_sequence)\n",
        "        print(f\"Sequence: {sequence_str}\")\n",
        "        print(f\"Constraints: {self.current_constraints}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dymw5lttcwV4"
      },
      "source": [
        "## A custom policy network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Cb9jg264cx-T"
      },
      "outputs": [],
      "source": [
        "class NUADFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom feature extractor for NUAD sequences and constraints\n",
        "    \"\"\"\n",
        "    def __init__(self, observation_space):\n",
        "        super().__init__()\n",
        "\n",
        "        # Extract dimensions from observation space\n",
        "        self.sequence_length = observation_space['sequence'].shape[1]\n",
        "        self.num_constraints = observation_space['constraint_values'].shape[0]\n",
        "\n",
        "        # CNN for sequence processing\n",
        "        self.sequence_features = nn.Sequential(\n",
        "            nn.Conv1d(4, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(output_size=8),  # Reduce to fixed size\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # MLP for constraint processing\n",
        "        self.constraint_features = nn.Sequential(\n",
        "            nn.Linear(self.num_constraints, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combined features\n",
        "        sequence_features_size = 64 * 8  # channels * adaptive pooling size\n",
        "        self.combined_layer = nn.Sequential(\n",
        "            nn.Linear(sequence_features_size + 32, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Feature dimension output for SB3\n",
        "        self.features_dim = 128\n",
        "\n",
        "    def forward(self, observations):\n",
        "        # Process sequence features (CNN)\n",
        "        seq_features = self.sequence_features(observations['sequence'])\n",
        "\n",
        "        # Process constraint features (MLP)\n",
        "        constraint_features = self.constraint_features(observations['constraint_values'])\n",
        "\n",
        "        # Combine features\n",
        "        combined = th.cat([seq_features, constraint_features], dim=1)\n",
        "        return self.combined_layer(combined)\n",
        "\n",
        "class NUADPolicy(ActorCriticPolicy):\n",
        "    \"\"\"\n",
        "    Custom policy for NUAD environment\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Pass our custom feature extractor to the parent class\n",
        "        kwargs[\"features_extractor_class\"] = NUADFeatureExtractor\n",
        "        super().__init__(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn12vjidc3fM"
      },
      "source": [
        "## A simplified training script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eml_zjSrc2rI",
        "outputId": "7e76fc23-9dae-40e7-c73e-09561595af87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Logging to ./nuad_ppo_tensorboard/PPO_1\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 118      |\n",
            "|    ep_rew_mean     | 39       |\n",
            "| time/              |          |\n",
            "|    fps             | 523      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 1024     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 133         |\n",
            "|    ep_rew_mean          | 45.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 236         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 2048        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014598599 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.77       |\n",
            "|    explained_variance   | -0.0733     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.338       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0297     |\n",
            "|    value_loss           | 2.62        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 136         |\n",
            "|    ep_rew_mean          | 47.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 221         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 3072        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011857193 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.76       |\n",
            "|    explained_variance   | 0.463       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.912       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0169     |\n",
            "|    value_loss           | 4.33        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 136         |\n",
            "|    ep_rew_mean          | 48.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 197         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010951731 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.73       |\n",
            "|    explained_variance   | 0.593       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.564       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    value_loss           | 4.29        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 144         |\n",
            "|    ep_rew_mean          | 54.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 196         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 5120        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013352787 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.67       |\n",
            "|    explained_variance   | 0.725       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.91        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0263     |\n",
            "|    value_loss           | 5.29        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 153         |\n",
            "|    ep_rew_mean          | 61.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 192         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 31          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013987478 |\n",
            "|    clip_fraction        | 0.195       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.59       |\n",
            "|    explained_variance   | 0.866       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.875       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0247     |\n",
            "|    value_loss           | 6.14        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 159         |\n",
            "|    ep_rew_mean          | 66.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 183         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 39          |\n",
            "|    total_timesteps      | 7168        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015324491 |\n",
            "|    clip_fraction        | 0.166       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.49       |\n",
            "|    explained_variance   | 0.889       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.6         |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0294     |\n",
            "|    value_loss           | 8.9         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 165         |\n",
            "|    ep_rew_mean          | 72          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 184         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 44          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017449021 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.36       |\n",
            "|    explained_variance   | 0.818       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.35        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0202     |\n",
            "|    value_loss           | 11.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 188         |\n",
            "|    ep_rew_mean          | 87.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 180         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 51          |\n",
            "|    total_timesteps      | 9216        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013092965 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.24       |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.79        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0164     |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 205         |\n",
            "|    ep_rew_mean          | 100         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 181         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017466096 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -4.16       |\n",
            "|    explained_variance   | 0.93        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.4         |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0221     |\n",
            "|    value_loss           | 12.6        |\n",
            "-----------------------------------------\n",
            "Step 1: Action=[8 0], Reward=0.6299\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [ 0.13036299 -1.78536317 -0.78883199  0.94502378  2.75778072]\n",
            "Step 2: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-0.36963701 -1.68642734 -0.81762232  0.85446595  2.25778072]\n",
            "Step 3: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-0.86963701 -1.58749152 -0.84641265  0.76390811  1.75778072]\n",
            "Step 4: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-1.36963701 -1.48855569 -0.87520298  0.67335028  1.25778072]\n",
            "Step 5: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-1.86963701 -1.38961987 -0.90399331  0.58279244  0.75778072]\n",
            "Step 6: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-2.36963701 -1.29068404 -0.93278364  0.4922346   0.25778072]\n",
            "Step 7: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-2.86963701 -1.19174822 -0.96157398  0.40167677 -0.24221928]\n",
            "Step 8: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-3.36963701 -1.0928124  -0.99036431  0.31111893 -0.74221928]\n",
            "Step 9: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-3.86963701 -0.99387657 -1.01915464  0.22056109 -1.24221928]\n",
            "Step 10: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-4.36963701 -0.89494075 -1.04794497  0.13000326 -1.74221928]\n",
            "Step 11: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-4.86963701 -0.79600492 -1.0767353   0.03944542 -2.24221928]\n",
            "Step 12: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-5.36963701 -0.6970691  -1.10552563 -0.05111241 -2.74221928]\n",
            "Step 13: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-5.86963701 -0.59813327 -1.13431597 -0.14167025 -3.24221928]\n",
            "Step 14: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-6.36963701 -0.49919745 -1.1631063  -0.23222809 -3.74221928]\n",
            "Step 15: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-6.86963701 -0.40026162 -1.19189663 -0.32278592 -4.24221928]\n",
            "Step 16: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-7.36963701 -0.3013258  -1.22068696 -0.41334376 -4.74221928]\n",
            "Step 17: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-7.86963701 -0.20238997 -1.24947729 -0.50390159 -5.24221928]\n",
            "Step 18: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-8.36963701 -0.10345415 -1.27826762 -0.59445943 -5.74221928]\n",
            "Step 19: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-8.86963701e+00 -4.51832415e-03 -1.30705796e+00 -6.85017267e-01\n",
            " -6.24221928e+00]\n",
            "Step 20: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-9.36963701  0.0944175  -1.33584829 -0.7755751  -6.74221928]\n",
            "Step 21: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-9.86963701  0.19335333 -1.36463862 -0.86613294 -7.24221928]\n",
            "Step 22: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-10.36963701   0.29228915  -1.39342895  -0.95669078  -7.74221928]\n",
            "Step 23: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-10.86963701   0.39122497  -1.42221928  -1.04724861  -8.24221928]\n",
            "Step 24: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-11.36963701   0.4901608   -1.45100961  -1.13780645  -8.74221928]\n",
            "Step 25: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-11.86963701   0.58909662  -1.47979995  -1.22836428  -9.24221928]\n",
            "Step 26: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-12.36963701   0.68803245  -1.50859028  -1.31892212  -9.74221928]\n",
            "Step 27: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-12.86963701   0.78696827  -1.53738061  -1.40947996 -10.24221928]\n",
            "Step 28: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-13.36963701   0.8859041   -1.56617094  -1.50003779 -10.74221928]\n",
            "Step 29: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-13.86963701   0.98483992  -1.59496127  -1.59059563 -11.24221928]\n",
            "Step 30: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-14.36963701   1.08377575  -1.6237516   -1.68115347 -11.74221928]\n",
            "Step 31: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-14.86963701   1.18271157  -1.65254194  -1.7717113  -12.24221928]\n",
            "Step 32: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-15.36963701   1.2816474   -1.68133227  -1.86226914 -12.74221928]\n",
            "Step 33: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-15.86963701   1.38058322  -1.7101226   -1.95282697 -13.24221928]\n",
            "Step 34: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-16.36963701   1.47951905  -1.73891293  -2.04338481 -13.74221928]\n",
            "Step 35: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-16.86963701   1.57845487  -1.76770326  -2.13394265 -14.24221928]\n",
            "Step 36: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-17.36963701   1.6773907   -1.79649359  -2.22450048 -14.74221928]\n",
            "Step 37: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-17.86963701   1.77632652  -1.82528393  -2.31505832 -15.24221928]\n",
            "Step 38: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-18.36963701   1.87526234  -1.85407426  -2.40561616 -15.74221928]\n",
            "Step 39: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-18.86963701   1.97419817  -1.88286459  -2.49617399 -16.24221928]\n",
            "Step 40: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-19.36963701   2.07313399  -1.91165492  -2.58673183 -16.74221928]\n",
            "Step 41: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-19.86963701   2.17206982  -1.94044525  -2.67728966 -17.24221928]\n",
            "Step 42: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-20.36963701   2.27100564  -1.96923558  -2.7678475  -17.74221928]\n",
            "Step 43: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-20.86963701   2.36994147  -1.99802592  -2.85840534 -18.24221928]\n",
            "Step 44: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-21.36963701   2.46887729  -2.02681625  -2.94896317 -18.74221928]\n",
            "Step 45: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-21.86963701   2.56781312  -2.05560658  -3.03952101 -19.24221928]\n",
            "Step 46: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-22.36963701   2.66674894  -2.08439691  -3.13007884 -19.74221928]\n",
            "Step 47: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-22.86963701   2.76568477  -2.11318724  -3.22063668 -20.24221928]\n",
            "Step 48: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-23.36963701   2.86462059  -2.14197757  -3.31119452 -20.74221928]\n",
            "Step 49: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-23.86963701   2.96355642  -2.17076791  -3.40175235 -21.24221928]\n",
            "Step 50: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-24.36963701   3.06249224  -2.19955824  -3.49231019 -21.74221928]\n",
            "Step 51: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-24.86963701   3.16142807  -2.22834857  -3.58286803 -22.24221928]\n",
            "Step 52: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-25.36963701   3.26036389  -2.2571389   -3.67342586 -22.74221928]\n",
            "Step 53: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-25.86963701   3.35929971  -2.28592923  -3.7639837  -23.24221928]\n",
            "Step 54: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-26.36963701   3.45823554  -2.31471956  -3.85454153 -23.74221928]\n",
            "Step 55: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-26.86963701   3.55717136  -2.3435099   -3.94509937 -24.24221928]\n",
            "Step 56: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-27.36963701   3.65610719  -2.37230023  -4.03565721 -24.74221928]\n",
            "Step 57: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-27.86963701   3.75504301  -2.40109056  -4.12621504 -25.24221928]\n",
            "Step 58: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-28.36963701   3.85397884  -2.42988089  -4.21677288 -25.74221928]\n",
            "Step 59: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-28.86963701   3.95291466  -2.45867122  -4.30733072 -26.24221928]\n",
            "Step 60: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-29.36963701   4.05185049  -2.48746155  -4.39788855 -26.74221928]\n",
            "Step 61: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-29.86963701   4.15078631  -2.51625189  -4.48844639 -27.24221928]\n",
            "Step 62: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-30.36963701   4.24972214  -2.54504222  -4.57900422 -27.74221928]\n",
            "Step 63: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-30.86963701   4.34865796  -2.57383255  -4.66956206 -28.24221928]\n",
            "Step 64: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-31.36963701   4.44759379  -2.60262288  -4.7601199  -28.74221928]\n",
            "Step 65: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-31.86963701   4.54652961  -2.63141321  -4.85067773 -29.24221928]\n",
            "Step 66: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-32.36963701   4.64546543  -2.66020354  -4.94123557 -29.74221928]\n",
            "Step 67: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-32.86963701   4.74440126  -2.68899388  -5.0317934  -30.24221928]\n",
            "Step 68: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-33.36963701   4.84333708  -2.71778421  -5.12235124 -30.74221928]\n",
            "Step 69: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-33.86963701   4.94227291  -2.74657454  -5.21290908 -31.24221928]\n",
            "Step 70: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-34.36963701   5.04120873  -2.77536487  -5.30346691 -31.74221928]\n",
            "Step 71: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-34.86963701   5.14014456  -2.8041552   -5.39402475 -32.24221928]\n",
            "Step 72: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-35.36963701   5.23908038  -2.83294553  -5.48458259 -32.74221928]\n",
            "Step 73: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-35.86963701   5.33801621  -2.86173587  -5.57514042 -33.24221928]\n",
            "Step 74: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-36.36963701   5.43695203  -2.8905262   -5.66569826 -33.74221928]\n",
            "Step 75: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-36.86963701   5.53588786  -2.91931653  -5.75625609 -34.24221928]\n",
            "Step 76: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-37.36963701   5.63482368  -2.94810686  -5.84681393 -34.74221928]\n",
            "Step 77: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-37.86963701   5.73375951  -2.97689719  -5.93737177 -35.24221928]\n",
            "Step 78: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-38.36963701   5.83269533  -3.00568752  -6.0279296  -35.74221928]\n",
            "Step 79: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-38.86963701   5.93163116  -3.03447786  -6.11848744 -36.24221928]\n",
            "Step 80: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-39.36963701   6.03056698  -3.06326819  -6.20904528 -36.74221928]\n",
            "Step 81: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-39.86963701   6.1295028   -3.09205852  -6.29960311 -37.24221928]\n",
            "Step 82: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-40.36963701   6.22843863  -3.12084885  -6.39016095 -37.74221928]\n",
            "Step 83: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-40.86963701   6.32737445  -3.14963918  -6.48071878 -38.24221928]\n",
            "Step 84: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-41.36963701   6.42631028  -3.17842951  -6.57127662 -38.74221928]\n",
            "Step 85: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-41.86963701   6.5252461   -3.20721985  -6.66183446 -39.24221928]\n",
            "Step 86: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-42.36963701   6.62418193  -3.23601018  -6.75239229 -39.74221928]\n",
            "Step 87: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-42.86963701   6.72311775  -3.26480051  -6.84295013 -40.24221928]\n",
            "Step 88: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-43.36963701   6.82205358  -3.29359084  -6.93350797 -40.74221928]\n",
            "Step 89: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-43.86963701   6.9209894   -3.32238117  -7.0240658  -41.24221928]\n",
            "Step 90: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-44.36963701   7.01992523  -3.3511715   -7.11462364 -41.74221928]\n",
            "Step 91: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-44.86963701   7.11886105  -3.37996184  -7.20518147 -42.24221928]\n",
            "Step 92: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-45.36963701   7.21779688  -3.40875217  -7.29573931 -42.74221928]\n",
            "Step 93: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-45.86963701   7.3167327   -3.4375425   -7.38629715 -43.24221928]\n",
            "Step 94: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-46.36963701   7.41566853  -3.46633283  -7.47685498 -43.74221928]\n",
            "Step 95: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-46.86963701   7.51460435  -3.49512316  -7.56741282 -44.24221928]\n",
            "Step 96: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-47.36963701   7.61354017  -3.52391349  -7.65797065 -44.74221928]\n",
            "Step 97: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-47.86963701   7.712476    -3.55270383  -7.74852849 -45.24221928]\n",
            "Step 98: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-48.36963701   7.81141182  -3.58149416  -7.83908633 -45.74221928]\n",
            "Step 99: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-48.86963701   7.91034765  -3.61028449  -7.92964416 -46.24221928]\n",
            "Step 100: Action=[8 0], Reward=1.0204\n",
            "Sequence: GATTTTATAGAGGAGGCCAATAGTGTGCAG\n",
            "Constraints: [-49.36963701   8.00928347  -3.63907482  -8.020202   -46.74221928]\n",
            "Episode 1: Total Reward=101.6507, Steps=100\n",
            "Episode 2: Total Reward=102.0412, Steps=100\n",
            "Episode 3: Total Reward=101.8402, Steps=100\n",
            "Episode 4: Total Reward=102.0412, Steps=100\n",
            "Episode 5: Total Reward=101.7124, Steps=100\n",
            "Episode 6: Total Reward=101.7124, Steps=100\n",
            "Episode 7: Total Reward=102.0412, Steps=100\n",
            "Episode 8: Total Reward=102.0412, Steps=100\n",
            "Episode 9: Total Reward=101.6507, Steps=100\n",
            "Episode 10: Total Reward=101.8402, Steps=100\n",
            "Average Reward over 10 episodes: 101.8572\n"
          ]
        }
      ],
      "source": [
        "def train_nuad_policy(total_timesteps=100000):\n",
        "    # Create environment\n",
        "    env = MockNUADEnv(sequence_length=30, num_constraints=5)\n",
        "\n",
        "    # Initialize PPO model with custom policy\n",
        "    model = PPO(\n",
        "        NUADPolicy,\n",
        "        env,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=1024,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        verbose=1,\n",
        "        tensorboard_log=\"./nuad_ppo_tensorboard/\"\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(\"nuad_policy_model\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_policy(model, env, num_episodes=10):\n",
        "    \"\"\"Evaluate the trained policy\"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < 100:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            # Print progress\n",
        "            if episode == 0:\n",
        "                print(f\"Step {steps}: Action={action}, Reward={reward:.4f}\")\n",
        "                env.render()\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        print(f\"Episode {episode+1}: Total Reward={total_reward:.4f}, Steps={steps}\")\n",
        "\n",
        "    print(f\"Average Reward over {num_episodes} episodes: {np.mean(rewards):.4f}\")\n",
        "    return rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train policy\n",
        "    model = train_nuad_policy(total_timesteps=10000)  # Small number for testing\n",
        "\n",
        "    # Evaluate policy\n",
        "    env = MockNUADEnv()\n",
        "    evaluate_policy(model, env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlD3FM85dAyv"
      },
      "source": [
        "## A realistic reward policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f-CgmhyHdFrT"
      },
      "outputs": [],
      "source": [
        "def realistic_nuad_reward(old_sequence, new_sequence, position, new_nucleotide):\n",
        "    \"\"\"\n",
        "    More realistic reward function that mimics NUAD constraints\n",
        "\n",
        "    This is a placeholder for NUAD's actual constraint evaluation\n",
        "    \"\"\"\n",
        "    reward = 0\n",
        "\n",
        "    # Example constraint types to simulate:\n",
        "\n",
        "    # 1. Base pairing stability (GC pairs are more stable than AT pairs)\n",
        "    # Check if we've created or broken complementary base pairs\n",
        "    for i in range(len(old_sequence)):\n",
        "        if i != position and i % 10 == (position % 10):  # Simulate complementary positions\n",
        "            # GC pair is worth more than AT pair\n",
        "            old_pair_value = 2 if (old_sequence[position] + old_sequence[i]) % 3 == 0 else 1\n",
        "            new_pair_value = 2 if (new_nucleotide + old_sequence[i]) % 3 == 0 else 1\n",
        "            reward += new_pair_value - old_pair_value\n",
        "\n",
        "    # 2. Sequence diversity (avoid repetitive sequences)\n",
        "    old_window = old_sequence[max(0, position-2):min(len(old_sequence), position+3)]\n",
        "    new_window = old_sequence.copy()\n",
        "    new_window[position] = new_nucleotide\n",
        "    new_window = new_window[max(0, position-2):min(len(old_sequence), position+3)]\n",
        "\n",
        "    old_diversity = len(set(old_window))\n",
        "    new_diversity = len(set(new_window))\n",
        "    reward += 0.5 * (new_diversity - old_diversity)\n",
        "\n",
        "    # 3. Target structure match (simplified)\n",
        "    # Simulate a target structure as alternating nucleotides\n",
        "    if position % 2 == 0 and new_nucleotide % 2 == 0:\n",
        "        reward += 0.5\n",
        "    elif position % 2 == 1 and new_nucleotide % 2 == 1:\n",
        "        reward += 0.5\n",
        "\n",
        "    return reward\n",
        "\n",
        "# Update the step method in MockNUADEnv to use this reward function\n",
        "def step(self, action):\n",
        "    # Unpack action\n",
        "    position, nucleotide = action\n",
        "\n",
        "    # Save old sequence for reward calculation\n",
        "    old_sequence = self.current_sequence.copy()\n",
        "\n",
        "    # Apply action\n",
        "    self.current_sequence[position] = nucleotide\n",
        "\n",
        "    # Calculate reward using the more realistic function\n",
        "    reward = realistic_nuad_reward(old_sequence, self.current_sequence, position, nucleotide)\n",
        "\n",
        "    # Update constraints based on the reward (to keep tracking progress)\n",
        "    for i in range(self.num_constraints):\n",
        "        self.current_constraints[i] -= reward / self.num_constraints\n",
        "\n",
        "    # Get new observation\n",
        "    observation = self._get_observation()\n",
        "\n",
        "    # Determine if done (when all constraints are satisfied)\n",
        "    done = all(c < -4.0 for c in self.current_constraints)\n",
        "\n",
        "    # Additional info\n",
        "    info = {\n",
        "        'constraint_values': self.current_constraints,\n",
        "        'improvement': reward\n",
        "    }\n",
        "\n",
        "    return observation, reward, done, False, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i--7X_-ff_G"
      },
      "source": [
        "## Step 6: Prepare for Integration with the Real NUAD Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pUAdDoG5fj8D"
      },
      "outputs": [],
      "source": [
        "class RLPolicyManager:\n",
        "    \"\"\"\n",
        "    Manager class for RL policies in NUAD\n",
        "\n",
        "    This will provide a clean interface between your RL implementation\n",
        "    and the actual NUAD environment when it's ready.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path=None):\n",
        "        self.model = None\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "\n",
        "    def train(self, env, total_timesteps=100000, save_path=\"nuad_policy_model\"):\n",
        "        \"\"\"Train a new policy on the provided environment\"\"\"\n",
        "        self.model = PPO(\n",
        "            NUADPolicy,\n",
        "            env,\n",
        "            learning_rate=3e-4,\n",
        "            n_steps=1024,\n",
        "            batch_size=64,\n",
        "            n_epochs=10,\n",
        "            gamma=0.99,\n",
        "            verbose=1,\n",
        "            tensorboard_log=\"./nuad_rl_logs/\"\n",
        "        )\n",
        "\n",
        "        self.model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "        if save_path:\n",
        "            self.model.save(save_path)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load a trained policy\"\"\"\n",
        "        self.model = PPO.load(model_path)\n",
        "        return self.model\n",
        "\n",
        "    def predict(self, observation):\n",
        "        \"\"\"Predict action based on observation\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model loaded. Call train() or load_model() first.\")\n",
        "\n",
        "        action, _ = self.model.predict(observation, deterministic=True)\n",
        "        return action\n",
        "\n",
        "    def search_for_sequences(self, env, max_steps=100):\n",
        "        \"\"\"\n",
        "        Run the policy to search for sequences\n",
        "\n",
        "        This mimics the interface of NUAD's search_for_sequences function\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model loaded. Call train() or load_model() first.\")\n",
        "\n",
        "        # Initialize environment\n",
        "        observation, _ = env.reset()\n",
        "\n",
        "        # Track sequence history\n",
        "        history = [env.current_sequence.copy()]\n",
        "        best_reward = float('-inf')\n",
        "        best_sequence = env.current_sequence.copy()\n",
        "\n",
        "        # Run policy until done or max steps\n",
        "        for step in range(max_steps):\n",
        "            action, _ = self.model.predict(observation, deterministic=False)  # Use stochasticity for exploration\n",
        "            observation, reward, done, _, info = env.step(action)\n",
        "\n",
        "            # Save history\n",
        "            history.append(env.current_sequence.copy())\n",
        "\n",
        "            # Track best sequence\n",
        "            if reward > best_reward:\n",
        "                best_reward = reward\n",
        "                best_sequence = env.current_sequence.copy()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Return results in a format similar to NUAD\n",
        "        result = {\n",
        "            'sequence': best_sequence,\n",
        "            'history': history,\n",
        "            'reward': best_reward,\n",
        "            'steps': step + 1,\n",
        "            'done': done\n",
        "        }\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tjB1-2LfpPS"
      },
      "source": [
        "## Step 7: Add Visualization for Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cWAezIQXgD0K"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "\n",
        "class TrainingVisualizationCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving and visualizing training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, log_dir, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.log_dir = log_dir\n",
        "        self.rewards = []\n",
        "        self.constraint_values = []\n",
        "\n",
        "    def _on_step(self):\n",
        "        # Record rewards and constraint values\n",
        "        if len(self.model.ep_info_buffer) > 0 and len(self.model.ep_info_buffer[-1]) > 0:\n",
        "            self.rewards.append(self.model.ep_info_buffer[-1][\"r\"])\n",
        "\n",
        "        # Visualize every 1000 steps\n",
        "        if self.n_calls % 1000 == 0:\n",
        "            self.visualize_progress()\n",
        "\n",
        "        return True\n",
        "\n",
        "    def visualize_progress(self):\n",
        "        \"\"\"Visualize training progress\"\"\"\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "        # Plot rewards\n",
        "        if len(self.rewards) > 0:\n",
        "            ax.plot(self.rewards, label='Episode Rewards')\n",
        "            ax.set_xlabel('Episodes')\n",
        "            ax.set_ylabel('Reward')\n",
        "            ax.set_title(f'Training Progress (Step {self.n_calls})')\n",
        "            ax.legend()\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(f\"{self.log_dir}/progress_{self.n_calls}.png\")\n",
        "        plt.close(fig)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
